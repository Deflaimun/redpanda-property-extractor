= Redpanda Configuration Properties 
:description: Redpanda configuration properties. 

== Broker

Broker configuration properties are applied individually to each broker in a cluster. 

IMPORTANT: After you change a broker-level property setting, you must restart the broker for the change to take effect. 

---

=== admin

Address and port of admin server

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* user

*Type:* #/definitions/model::broker_endpoint

*Default:* '{model::broker_endpoint(net::unresolved_address("127.0.0.1", 9644))}'

=== admin_api_doc_dir

Admin API doc directory

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* user

*Type:* string

*Default:* '/usr/share/redpanda/admin-api-doc'

=== admin_api_tls

TLS configuration for admin HTTP server

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* user

*Type:* #/definitions/endpoint_tls_config

*Default:* None

=== cloud_storage_cache_directory

Directory for archival cache. Should be present when `cloud_storage_enabled` is present

*Requires Restart:* Yes

*Nullable:* Yes

*Visibility:* user

*Type:* string

*Default:* None

=== crash_loop_limit

Maximum consecutive crashes (unclean shutdowns) allowed after which operator intervention is needed to startup the broker. Limit is not enforced in developer mode.

*Requires Restart:* Yes

*Nullable:* Yes

*Visibility:* user

*Type:* integer

*Accepted values:* [0, 4294967295]

*Default:* 5

=== data_directory

Place where redpanda will keep the data

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* user

*Type:* string

*Default:* None

=== developer_mode

Skips most of the checks performed at startup, not recomended for production use

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* False

=== emergency_disable_data_transforms

Override the cluster enablement setting and disable WebAssembly powered data transforms. Only used as an emergency shutoff button.

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* False

=== empty_seed_starts_cluster

If true, an empty seed_servers list will denote that this node should form a cluster. At most one node in the cluster should be configured configured with an empty seed_servers list. If no such configured node exists, or if configured to false, all nodes denoted by the seed_servers list must be identical among those nodes' configurations, and those nodes will form the initial cluster.

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* True

=== kafka_api

Address and port of an interface to listen for Kafka API requests

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* user

*Type:* #/definitions/config::broker_authn_endpoint

*Default:* {'address': 'net::unresolved_address("127.0.0.1", 9092)', 'authn_method': 'std::nullopt'}

=== kafka_api_tls

TLS configuration for Kafka API endpoint

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* user

*Type:* #/definitions/endpoint_tls_config

*Default:* None

=== memory_allocation_warning_threshold

Enables log messages for allocations greater than the given size.

*Requires Restart:* Yes

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Default:* '128_KiB + 1'

=== node_id

Unique id identifying a node in the cluster. If missing, a unique id will be assigned for this node when it joins the cluster

*Requires Restart:* Yes

*Nullable:* Yes

*Visibility:* user

*Type:* #/definitions/model::node_id

*Default:* None

=== rack

Rack identifier

*Requires Restart:* Yes

*Nullable:* Yes

*Visibility:* user

*Type:* #/definitions/model::rack_id

*Default:* None

=== recovery_mode_enabled

If true, start redpanda in "metadata only" mode, skipping loading user partitions and allowing only metadata operations.

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* False

=== rpc_server

IpAddress and port for RPC server

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* user

*Type:* #/definitions/net::unresolved_address

*Default:* 'net::unresolved_address("127.0.0.1", 33145)'

=== rpc_server_tls

TLS configuration for RPC server

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* user

*Type:* #/definitions/tls_config

*Default:* 'tls_config()'

=== seed_servers

List of the seed servers used to join current cluster. If the seed_server list is empty the node will be a cluster root and it will form a new cluster

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* user

*Type:* array

*Default:* None

=== storage_failure_injection_config_path

Path to the configuration file used for low level storage failure injection

*Requires Restart:* Yes

*Nullable:* Yes

*Visibility:* tunable

*Type:* string

*Default:* None

=== storage_failure_injection_enabled

If true, inject low level storage failures on the write path. **Not** for production usage.

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* False

=== upgrade_override_checks

Whether to violate safety checks when starting a redpanda version newer than the cluster's consensus version

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* False

=== verbose_logging_timeout_sec_max

Maximum duration in seconds for verbose (i.e. TRACE or DEBUG) logging. Values configured above this will be clamped. If null (the default) there is no limit. Can be overridded in the Admin API on a per-request basis.

*Requires Restart:* Yes

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17179869184, 17179869183]

*Default:* None



== Cluster Configuration

Cluster Configuration intro

=== abort_index_segment_size

Capacity (in number of txns) of an abort index segment

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 4294967295]

*Default:* 50000

=== abort_timed_out_transactions_interval_ms

How often look for the inactive transactions and abort them

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '10s'

=== admin_api_require_auth

Whether admin API clients must provide HTTP Basic authentication headers

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* False

=== aggregate_metrics

Enable aggregations of metrics returned by the prometheus '/metrics' endpoint. Metric aggregation is performed by summing the values of samples by labels. Aggregations are performed where it makes sense by the shard and/or partition labels.

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* boolean

*Default:* False

=== alter_topic_cfg_timeout_ms

Time to wait for entries replication in controller log when executing alter configuration request

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '5s'

=== append_chunk_size

Size of direct write operations to disk in bytes

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 16384

=== audit_client_max_buffer_size

Maximum number of bytes the internal audit client will allocate for audit log records. Disable and re-enable auditing for changes to take affect

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Default:* 16777216

=== audit_enabled

Enable/Disable audit logging.

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* False

=== audit_enabled_event_types

List of event classes that will be audited, options are: [management, produce, consume, describe, heartbeat, authenticate, admin, schema_registry]. Please refer to the documentation to know exactly which request(s) map to a particular audit event type.

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* array

*Default:* ['management', 'authenticate', 'admin']

=== audit_excluded_principals

List of user principals to exclude from auditing

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* array

*Default:* None

=== audit_excluded_topics

List of topics to exclude from auditing

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* array

*Default:* None

=== audit_log_num_partitions

Number of partitions for the internal audit log topic. Attempt to create topic is only performed if it doesn't already exist, disable and re-enable auditing for changes to take affect

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [-2147483648, 2147483647]

*Default:* 12

=== audit_log_replication_factor

Replication factor of the internal audit log topic. Attempt to create topic is only performed if it doesn't already exist, disable and re-enable auditing for changes to take affect.  If unset, defaults to `default_topic_replication`

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* integer

*Accepted values:* [-32768, 32767]

*Default:* None

=== audit_queue_drain_interval_ms

Frequency in which per shard audit logs are batched to client for write to audit log. Longer intervals allow for greater change for coalescing duplicates (great for high throughput auditing scenarios) but increase the risk of data loss during hard shutdowns.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '500ms'

=== audit_queue_max_buffer_size_per_shard

Maximum amount of memory allowed in the audit buffer per shard Once this value is reached, any request handlers that cannot enqueue audit messages will return a non retryable error to the client. Note that this only will occur when handling requests that are currently enabled for auditing.

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 1048576

=== auto_create_topics_enabled

Allow topic auto creation

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* False

=== cloud_storage_access_key

AWS access key

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* string

*Default:* None

=== cloud_storage_api_endpoint

Optional API endpoint

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* string

*Default:* None

=== cloud_storage_api_endpoint_port

TLS port override

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [-32768, 32767]

*Default:* 443

=== cloud_storage_attempt_cluster_restore_on_bootstrap

If set to `true`, when a cluster is started for the first time and there is cluster metadata in the configured cloud storage bucket, Redpanda automatically starts a cluster restore from that metadata. If using an automated method for deployment where it's not easy to predictably determine that a restore is needed, we recommend setting to `true`. Take care to ensure that in such deployments, a cluster bootstrap with a given bucket means that any previous cluster using that bucket is fully destroyed; otherwise tiered storage subsystems may interfere with each other.

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* False

=== cloud_storage_azure_adls_endpoint

Azure Data Lake Storage v2 endpoint override. Use when Hierarchical Namespaces are enabled on your storage account and you have set up a custom endpoint.

*Requires Restart:* Yes

*Nullable:* Yes

*Visibility:* user

*Type:* string

*Default:* None

=== cloud_storage_azure_adls_port

Azure Data Lake Storage v2 port override. Also see cloud_storage_azure_adls_endpoint.

*Requires Restart:* Yes

*Nullable:* Yes

*Visibility:* user

*Type:* integer

*Accepted values:* [0, 65535]

*Default:* None

=== cloud_storage_azure_container

The name of the Azure container to use with Tiered Storage. Note that the container must belong to 'cloud_storage_azure_storage_account'

*Requires Restart:* Yes

*Nullable:* Yes

*Visibility:* user

*Type:* string

*Default:* None

=== cloud_storage_azure_shared_key

The shared key to be used for Azure Shared Key authentication with the configured Azure storage account (see 'cloud_storage_azure_storage_account)'. Note that Redpanda expects this string to be Base64 encoded.

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* string

*Default:* None

=== cloud_storage_azure_storage_account

The name of the Azure storage account to use with Tiered Storage

*Requires Restart:* Yes

*Nullable:* Yes

*Visibility:* user

*Type:* string

*Default:* None

=== cloud_storage_backend

Optional cloud storage backend variant used to select API capabilities. If not supplied, will be inferred from other configuration parameters.

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* user

*Type:* #/definitions/model::cloud_storage_backend

*Default:* 'model::cloud_storage_backend::unknown'

=== cloud_storage_background_jobs_quota

The number of total requests that the cloud storage background jobs are allowed to make during one background housekeeping run. This is a per shard limit.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-2147483648, 2147483647]

*Default:* 5000

=== cloud_storage_bucket

AWS bucket that should be used to store data

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* string

*Default:* None

=== cloud_storage_cache_check_interval_ms

Minimum time between trims of tiered storage cache.  If a fetch operation requires trimming the cache, and the most recent trim was within this period, then trimming will be delayed until this period has elapsed

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '5s'

=== cloud_storage_cache_chunk_size

Size of chunks of segments downloaded into cloud storage cache. Reduces space usage by only downloading the necessary chunk from a segment.

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 18446744073709551615]

*Default:* 16777216

=== cloud_storage_cache_max_objects

Maximum number of objects that may be held in the tiered storage cache.  This applies simultaneously with `cloud_storage_cache_size`, and which ever limit is hit first will drive trimming of the cache.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 4294967295]

*Default:* 100000

=== cloud_storage_cache_size

Max size of archival cache

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [0, 18446744073709551615]

*Default:* 0

=== cloud_storage_cache_size_percent

The maximum size of the archival cache as a percentage of unreserved disk space (see disk_reservation_percent). The default value for this option is tuned for a shared disk configuration. When using a dedicated cache disk consider increasing the value.

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* number

*Default:* 20.0

=== cloud_storage_chunk_eviction_strategy

Selects a strategy for evicting unused cache chunks.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* #/definitions/model::cloud_storage_chunk_eviction_strategy

*Default:* 'model::cloud_storage_chunk_eviction_strategy::eager'

=== cloud_storage_chunk_prefetch

Number of chunks to prefetch ahead of every downloaded chunk

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 65535]

*Default:* 0

=== cloud_storage_cluster_metadata_num_consumer_groups_per_upload

Number of groups to upload in a single snapshot object during consumer offsets upload. Setting a lower value will mean a larger number of smaller snapshots are uploaded.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 1000

=== cloud_storage_cluster_metadata_retries

Number of attempts metadata operations may be retried.

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-32768, 32767]

*Default:* 5

=== cloud_storage_cluster_metadata_upload_interval_ms

Time interval to wait between cluster metadata uploads.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '1h'

=== cloud_storage_cluster_metadata_upload_timeout_ms

Timeout for cluster metadata uploads.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '60s'

=== cloud_storage_credentials_host

The hostname to connect to for retrieving role based credentials. Derived from cloud_storage_credentials_source if not set. Only required when using IAM role based access.

*Requires Restart:* Yes

*Nullable:* Yes

*Visibility:* tunable

*Type:* string

*Default:* None

=== cloud_storage_credentials_source

The source of credentials to connect to cloud services

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* user

*Type:* #/definitions/model::cloud_credentials_source

*Default:* 'model::cloud_credentials_source::config_file'

=== cloud_storage_disable_chunk_reads

Disable chunk reads and switch back to legacy mode where full segments are downloaded.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* False

=== cloud_storage_disable_metadata_consistency_checks

Disable all metadata consistency checks. This will allow redpanda to replay logs with inconsistent tiered-storage metadata. Normally, this option should be disabled.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* True

=== cloud_storage_disable_read_replica_loop_for_tests

Begins the read replica sync loop in tiered-storage-enabled topic partitions. The property exists to simplify testing and shouldn't be set in production.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* False

=== cloud_storage_disable_tls

Disable TLS for all S3 connections

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* False

=== cloud_storage_disable_upload_consistency_checks

Disable all upload consistency checks. This will allow redpanda to upload logs with gaps and replicate metadata with consistency violations. Normally, this options should be disabled.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* False

=== cloud_storage_disable_upload_loop_for_tests

Begins the upload loop in tiered-storage-enabled topic partitions. The property exists to simplify testing and shouldn't be set in production.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* False

=== cloud_storage_enable_compacted_topic_reupload

Enable re-uploading data for compacted topics

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* True

=== cloud_storage_enable_remote_read

Default remote read config value for new topics

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* False

=== cloud_storage_enable_remote_write

Default remote write value for new topics

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* False

=== cloud_storage_enable_scrubbing

Enable scrubbing of cloud storage partitions. The scrubber validates the integrity of data and metadata uploaded to cloud storage

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* False

=== cloud_storage_enable_segment_merging

Enables adjacent segment merging. The segments are reuploaded if there is an opportunity for that and if it will improve the tiered-storage performance

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* True

=== cloud_storage_enabled

Enable archival storage

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* False

=== cloud_storage_full_scrub_interval_ms

Time interval between a final scrub and thte next scrub

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '12h'

=== cloud_storage_garbage_collect_timeout_ms

Timeout for running the cloud storage garbage collection (ms)

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '30s'

=== cloud_storage_graceful_transfer_timeout_ms

Time limit on waiting for uploads to complete before a leadership transfer.  If this is null, leadership transfers will proceed without waiting.

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '5s'

=== cloud_storage_housekeeping_interval_ms

Interval for cloud storage housekeeping tasks

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '5min'

=== cloud_storage_hydrated_chunks_per_segment_ratio

The maximum number of chunks per segment that can be hydrated at a time. Above this number, unused chunks will be trimmed.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* number

*Default:* 0.7

=== cloud_storage_hydration_timeout_ms

Duration to wait for a hydration request to be fulfilled, if hydration is not completed within this time, the consumer will be notified with a timeout error.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '600s'

=== cloud_storage_idle_threshold_rps

The cloud storage request rate threshold for idle state detection. If the average request rate for the configured period is lower than this threshold the cloud storage is considered being idle.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* number

*Default:* 10.0

=== cloud_storage_idle_timeout_ms

Timeout used to detect idle state of the cloud storage API. If the average cloud storage request rate is below this threshold for a configured amount of time the cloud storage is considered idle and the housekeeping jobs are started.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '10s'

=== cloud_storage_initial_backoff_ms

Initial backoff time for exponential backoff algorithm (ms)

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '100ms'

=== cloud_storage_manifest_cache_size

Amount of memory that can be used to handle tiered-storage metadata

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 1048576

=== cloud_storage_manifest_cache_ttl_ms

The time interval that determines how long the materialized manifest can stay in cache under contention. This parameter is used for performance tuning. When the spillover manifest is materialized and stored in cache and the cache needs to evict it it will use 'cloud_storage_materialized_manifest_ttl_ms' value as a timeout. The cursor that uses the spillover manifest uses this value as a TTL interval after which it stops referencing the manifest making it available for eviction. This only affects spillover manifests under contention.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '10s'

=== cloud_storage_manifest_max_upload_interval_sec

Wait at least this long between partition manifest uploads. Actual time between uploads may be greater than this interval. If this is null, metadata will be updated after each segment upload.

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17179869184, 17179869183]

*Default:* '60s'

=== cloud_storage_manifest_upload_timeout_ms

Manifest upload timeout (ms)

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '10s'

=== cloud_storage_max_concurrent_hydrations_per_shard

Maximum concurrent segment hydrations of remote data per CPU core.  If unset, value of `cloud_storage_max_connections / 2` is used, which means that half of available S3 bandwidth could be used to download data from S3. If the cloud storage cache is empty every new segment reader will require a download. This will lead to 1:1 mapping between number of partitions scanned by the fetch request and number of parallel downloads. If this value is too large the downloads can affect other workloads. In case of any problem caused by the tiered-storage reads this value can be lowered. This will only affect segment hydrations (downloads) but won't affect cached segments. If fetch request is reading from the tiered-storage cache its concurrency will only be limited by available memory.

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 4294967295]

*Default:* None

=== cloud_storage_max_connection_idle_time_ms

Max https connection idle time (ms)

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '5s'

=== cloud_storage_max_connections

Max number of simultaneous connections to S3 per shard (includes connections used for both uploads and downloads)

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [-32768, 32767]

*Default:* 20

=== cloud_storage_max_segment_readers_per_shard

Maximum concurrent I/O cursors of materialized remote segments per CPU core.  If unset, value of `topic_partitions_per_shard` is used, i.e. one segment reader per partition if the shard is at its maximum partition capacity.  These readers are cachedacross Kafka consume requests and store a readahead buffer.

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 4294967295]

*Default:* None

=== cloud_storage_max_segments_pending_deletion_per_partition

The per-partition limit for the number of segments pending deletion from the cloud. Segments can be deleted due to retention or compaction. If this limit is breached and deletion fails, then segments will be orphaned in the cloud and will have to be removed manually

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 5000

=== cloud_storage_max_throughput_per_shard

Max throughput used by tiered-storage per shard in bytes per second. This value is an upper bound of the throughput available to the tiered-storage subsystem. This parameter is intended to be used as a safeguard and in tests when we need to set precise throughput value independent of actual storage media. Please use 'cloud_storage_throughput_limit_percent' instead of this parameter in the production environment.

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Default:* 1073741824

=== cloud_storage_metadata_sync_timeout_ms

Timeout for SI metadata synchronization

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '10s'

=== cloud_storage_min_chunks_per_segment_threshold

The minimum number of chunks per segment for trimming to be enabled. If the number of chunks in a segment is below this threshold, the segment is small enough that all chunks in it can be hydrated at any given time

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 18446744073709551615]

*Default:* 5

=== cloud_storage_partial_scrub_interval_ms

Time interval between two partial scrubs of the same partition

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '1h'

=== cloud_storage_readreplica_manifest_sync_timeout_ms

Timeout to check if new data is available for partition in S3 for read replica

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '30s'

=== cloud_storage_recovery_temporary_retention_bytes_default

Retention in bytes for topics created during automated recovery

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 1073741824

=== cloud_storage_region

AWS region that houses the bucket used for storage

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* string

*Default:* None

=== cloud_storage_roles_operation_timeout_ms

Timeout for IAM role related operations (ms)

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '30s'

=== cloud_storage_scrubbing_interval_jitter_ms

Jitter applied to the cloud storage scrubbing interval.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '10min'

=== cloud_storage_secret_key

AWS secret key

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* string

*Default:* None

=== cloud_storage_segment_max_upload_interval_sec

Time that segment can be kept locally without uploading it to the remote storage (sec)

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17179869184, 17179869183]

*Default:* '1h'

=== cloud_storage_segment_size_min

Smallest acceptable segment size in the cloud storage. Default: cloud_storage_segment_size_target/2

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Default:* None

=== cloud_storage_segment_size_target

Desired segment size in the cloud storage. Default: segment.bytes

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Default:* None

=== cloud_storage_segment_upload_timeout_ms

Log segment upload timeout (ms)

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '30s'

=== cloud_storage_spillover_manifest_max_segments

Maximum number of elements in the spillover manifest that can be offloaded to the cloud storage. This property is similar to 'cloud_storage_spillover_manifest_size' but it triggers spillover based on number of segments instead of the size of the manifest in bytes. The property exists to simplify testing and shouldn't be set in the production environment

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Default:* None

=== cloud_storage_spillover_manifest_size

The size of the manifest which can be offloaded to the cloud. If the size of the local manifest stored in redpanda exceeds cloud_storage_spillover_manifest_size x2 the spillover mechanism will split the manifest into two parts and one of them will be uploaded to S3.

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Default:* 65536

=== cloud_storage_throughput_limit_percent

Max throughput used by tiered-storage per node expressed as a percentage of the disk bandwidth. If the server has several disks Redpanda will take into account only the one which is used to store tiered-storage cache. Note that even if the tiered-storage is allowed to use full bandwidth of the disk (100%) it won't necessary use it in full. The actual usage depend on your workload and the state of the tiered-storage cache. This parameter is a safeguard that prevents tiered-storage from using too many system resources and not a performance tuning knob.

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Default:* 50

=== cloud_storage_topic_purge_grace_period_ms

Grace period during which the purger will refuse to purge the topic.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '30s'

=== cloud_storage_trust_file

Path to certificate that should be used to validate server certificate during TLS handshake

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* string

*Default:* None

=== cloud_storage_upload_ctrl_d_coeff

derivative coefficient for upload PID controller.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* number

*Default:* 0.0

=== cloud_storage_upload_ctrl_max_shares

maximum number of IO and CPU shares that archival upload can use

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-32768, 32767]

*Default:* 1000

=== cloud_storage_upload_ctrl_min_shares

minimum number of IO and CPU shares that archival upload can use

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-32768, 32767]

*Default:* 100

=== cloud_storage_upload_ctrl_p_coeff

proportional coefficient for upload PID controller

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* number

*Default:* -2.0

=== cloud_storage_upload_ctrl_update_interval_ms



*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '60s'

=== cloud_storage_upload_loop_initial_backoff_ms

Initial backoff interval when there is nothing to upload for a partition (ms)

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '100ms'

=== cloud_storage_upload_loop_max_backoff_ms

Max backoff interval when there is nothing to upload for a partition (ms)

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '10s'

=== cluster_id

Cluster identifier

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* None

*Type:* string

*Default:* None

=== compacted_log_segment_size

How large in bytes should each compacted log segment be (default 256MiB)

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 18446744073709551615]

*Default:* 268435456

=== compaction_ctrl_backlog_size

target backlog size for compaction controller. if not set compaction target compaction backlog would be equal to 

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Default:* None

=== compaction_ctrl_d_coeff

derivative coefficient for compaction PID controller.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* number

*Default:* 0.2

=== compaction_ctrl_i_coeff

integral coefficient for compaction PID controller.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* number

*Default:* 0.0

=== compaction_ctrl_max_shares

maximum number of IO and CPU shares that compaction process can use

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-32768, 32767]

*Default:* 1000

=== compaction_ctrl_min_shares

minimum number of IO and CPU shares that compaction process can use

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-32768, 32767]

*Default:* 10

=== compaction_ctrl_p_coeff

proportional coefficient for compaction PID controller. This has to be negative since compaction backlog should decrease when number of compaction shares increases

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* number

*Default:* -12.5

=== compaction_ctrl_update_interval_ms



*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '30s'

=== controller_backend_housekeeping_interval_ms

Interval between iterations of controller backend housekeeping loop

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '1s'

=== controller_log_accummulation_rps_capacity_acls_and_users_operations

Maximum capacity of rate limit accumulationin controller acls and users operations limit

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Default:* None

=== controller_log_accummulation_rps_capacity_configuration_operations

Maximum capacity of rate limit accumulationin controller configuration operations limit

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Default:* None

=== controller_log_accummulation_rps_capacity_move_operations

Maximum capacity of rate limit accumulationin controller move operations limit

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Default:* None

=== controller_log_accummulation_rps_capacity_node_management_operations

Maximum capacity of rate limit accumulationin controller node management operations limit

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Default:* None

=== controller_log_accummulation_rps_capacity_topic_operations

Maximum capacity of rate limit accumulationin controller topic operations limit

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Default:* None

=== controller_snapshot_max_age_sec

Max time that will pass before we make an attempt to create a controller snapshot, after a new controller command appears

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17179869184, 17179869183]

*Default:* '60s'

=== cpu_profiler_enabled

Enables cpu profiling for Redpanda

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* False

=== cpu_profiler_sample_period_ms

The sample period for the CPU profiler

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '100ms'

=== create_topic_timeout_ms

Timeout (ms) to wait for new topic creation

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* "2'000ms"

=== data_transforms_binary_max_size

The maximum size for a deployable WebAssembly binary that the broker can store.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 10485760

=== data_transforms_commit_interval_ms

The interval at which Data Transforms commits progress.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '3s'

=== data_transforms_enabled

Enables WebAssembly powered Data Transforms directly in the broker

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* False

=== data_transforms_logging_buffer_capacity_bytes

Buffer capacity for transform logs, per shard. Buffer occupancy is calculated as the total size of buffered (i.e. emitted but not yet produced) log messages.

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 102400

=== data_transforms_logging_flush_interval_ms

Flush interval for transform logs. When a timer expires, pending logs are collected and published to the transform_logs topic.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '500ms'

=== data_transforms_logging_line_max_bytes

Transform log lines will be truncate to this length. Truncation occurs after any character escaping.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 1024

=== data_transforms_per_core_memory_reservation

The amount of memory to reserve per core for Data Transform WebAssembly Virtual Machines. Memory is reserved on boot. The maximum number of functions that can be deployed to a cluster is equal to data_transforms_per_core_memory_reservation / data_transforms_per_function_memory_limit

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* user

*Type:* integer

*Default:* 20971520

=== data_transforms_per_function_memory_limit

The amount of memory to give an instance of a Data Transform WebAssembly Virtual Machine. The maximum number of functions that can be deployed to a cluster is equal to data_transforms_per_core_memory_reservation / data_transforms_per_function_memory_limit

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* user

*Type:* integer

*Default:* 2097152

=== data_transforms_runtime_limit_ms

The maximum amount of runtime for startup time of a data transform, and the time it takes for a single record to be transformed.

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '3s'

=== default_num_windows

Default number of quota tracking windows

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-32768, 32767]

*Default:* 10

=== default_topic_partitions

Default number of partitions per topic

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [-2147483648, 2147483647]

*Default:* 1

=== default_topic_replication

Default replication factor for new topics

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [-32768, 32767]

*Default:* 1

=== default_window_sec

Default quota tracking window size in milliseconds

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* 'std::chrono::milliseconds(1000)'

=== disable_batch_cache

Disable batch cache in log manager

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* False

=== disable_cluster_recovery_loop_for_tests

Disables the cluster recovery loop. The property exists to simplify testing and shouldn't be set in production.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* False

=== disable_metrics

Disable registering metrics exposed on the internal metrics endpoint (/metrics)

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* boolean

*Default:* False

=== disable_public_metrics

Disable registering metrics exposed on the public metrics endpoint (/public_metrics)

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* boolean

*Default:* False

=== disk_reservation_percent

The percentage of total disk capacity that Redpanda will avoid using. This applies both when cloud cache and log data share a disk, as well as when cloud cache uses a dedicated disk. It is recommended to not run disks near capacity to avoid blocking I/O due to low disk space, as well as avoiding performance issues associated with SSD garbage collection.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* number

*Default:* 25.0

=== enable_cluster_metadata_upload_loop

Enables the cluster metadata upload loop.

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* True

=== enable_controller_log_rate_limiting

Enables limiting of controller log write rate

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* False

=== enable_idempotence

Enable idempotent producer

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* True

=== enable_leader_balancer

Enable automatic leadership rebalancing

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* True

=== enable_metrics_reporter

Enable cluster metrics reporter

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* True

=== enable_mpx_extensions

Enable Redpanda extensions for MPX.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* False

=== enable_pid_file

Enable pid file. You probably don't want to change this.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* True

=== enable_rack_awareness

Enables rack-aware replica assignment

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* False

=== enable_sasl

Enable SASL authentication for Kafka connections, authorization is required. see also `kafka_enable_authorization`

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* False

=== enable_schema_id_validation

Enable Server Side Schema ID Validation.

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* #/definitions/pandaproxy::schema_registry::schema_id_validation_mode

*Default:* 'pandaproxy::schema_registry::schema_id_validation_mode::none'

=== enable_transactions

Enable transactions

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* True

=== enable_usage

Enables the usage tracking mechanism, storing windowed history of kafka/cloud_storage metrics over time

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* False

=== features_auto_enable

Whether new feature flags may auto-activate after upgrades (true) or must wait for manual activation via the admin API (false)

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* True

=== fetch_max_bytes

Maximum number of bytes returned in fetch request

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Default:* 57671680

=== fetch_read_strategy

The strategy used to fulfill fetch requests

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* model::fetch_read_strategy

*Default:* 'model::fetch_read_strategy::non_polling'

=== fetch_reads_debounce_timeout

Time to wait for next read in fetch request when requested min bytes wasn't reached

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '1ms'

=== fetch_session_eviction_timeout_ms

Minimum time before which unused session will get evicted from sessions. Maximum time after which inactive session will be deleted is two time given configuration valuecache

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '60s'

=== find_coordinator_timeout_ms

Time to wait for a response from tx_registry

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '2000ms'

=== group_initial_rebalance_delay

Extra delay (ms) added to rebalance phase to wait for new members

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '3s'

=== group_max_session_timeout_ms

The maximum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures. 

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '300s'

=== group_min_session_timeout_ms

The minimum allowed session timeout for registered consumers. Shorter timeouts result in quicker failure detection at the cost of more frequent consumer heartbeating, which can overwhelm broker resources.

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '6000ms'

=== group_new_member_join_timeout

Timeout for new member joins

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* "30'000ms"

=== group_offset_retention_check_ms

How often the system should check for expired group offsets.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '10min'

=== group_offset_retention_sec

Consumer group offset retention seconds. Offset retention can be disabled by setting this value to null.

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17179869184, 17179869183]

*Default:* '24h * 7'

=== group_topic_partitions

Number of partitions in the internal group membership topic

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-2147483648, 2147483647]

*Default:* 16

=== health_manager_tick_interval

How often the health manager runs

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '3min'

=== health_monitor_max_metadata_age

Max age of metadata cached in the health monitor of non controller node

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '10s'

=== http_authentication

A list of supported HTTP authentication mechanisms. `BASIC` and `OIDC` are allowed.

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* array

*Default:* ['BASIC']

=== id_allocator_batch_size

Id allocator allocates messages in batches (each batch is a one log record) and then serves requests from memory without touching the log until the batch is exhausted.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-32768, 32767]

*Default:* 1000

=== id_allocator_log_capacity

Capacity of the id_allocator log in number of batches. Once it reached id_allocator_stm truncates log's prefix.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-32768, 32767]

*Default:* 100

=== initial_retention_local_target_bytes_default

Initial local retention size target for partitions of topics with cloud storage write enabled. If no initial local target retention is configured all locally retained data will be delivered to learner when joining partition replica set

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* integer

*Default:* None

=== initial_retention_local_target_ms_default

Initial local retention time target for partitions of topics with cloud storage write enabled. If no initial local target retention is configured all locally retained data will be delivered to learner when joining partition replica set

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* None

=== internal_topic_replication_factor

Target replication factor for internal topics

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [-2147483648, 2147483647]

*Default:* 3

=== join_retry_timeout_ms

Time between cluster join retries in milliseconds

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '5s'

=== kafka_admin_topic_api_rate

Target quota rate (partition mutations per default_window_sec)

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* integer

*Accepted values:* [0, 4294967295]

*Default:* None

=== kafka_batch_max_bytes

Maximum size of a batch processed by server. If batch is compressed the limit applies to compressed batch size

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 4294967295]

*Default:* 1048576

=== kafka_client_group_byte_rate_quota

Per-group target produce quota byte rate (bytes per second). Client is considered part of the group if client_id contains clients_prefix

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* #/definitions/client_group_quota

*Default:* None

=== kafka_client_group_fetch_byte_rate_quota

Per-group target fetch quota byte rate (bytes per second). Client is considered part of the group if client_id contains clients_prefix

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* #/definitions/client_group_quota

*Default:* None

=== kafka_connection_rate_limit

Maximum connections per second for one core

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* integer

*Accepted values:* [-9223372036854775808, 9223372036854775807]

*Default:* None

=== kafka_connection_rate_limit_overrides

Overrides for specific ips for maximum connections per second for one core

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* array

*Default:* None

=== kafka_connections_max

Maximum number of Kafka client connections per broker

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* integer

*Accepted values:* [0, 4294967295]

*Default:* None

=== kafka_connections_max_overrides

Per-IP overrides of kafka connection count limit, list of <ip>:<count> strings

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* array

*Default:* None

=== kafka_connections_max_per_ip

Maximum number of Kafka client connections from each IP address, per broker

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* integer

*Accepted values:* [0, 4294967295]

*Default:* None

=== kafka_enable_authorization

Enable authorization for Kafka connections. Values:- `nil`: Ignored. Authorization is enabled with `enable_sasl: true`; `true`: authorization is required; `false`: authorization is disabled. See also: `enable_sasl` and `kafka_api[].authentication_method`

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* boolean

*Default:* None

=== kafka_enable_describe_log_dirs_remote_storage

Whether to include tiered storage as a special remote:// directory in DescribeLogDirs Kafka API requests.

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* True

=== kafka_enable_partition_reassignment

Enable the Kafka partition reassignment API

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* True

=== kafka_group_recovery_timeout_ms

Kafka group recovery timeout expressed in milliseconds

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* "30'000ms"

=== kafka_max_bytes_per_fetch

Limit fetch responses to this many bytes, even if total of partition bytes limits is higher

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 67108864

=== kafka_memory_batch_size_estimate_for_fetch

The size of the batch used to estimate memory consumption for Fetch requests, in bytes. Smaller sizes allow more concurrent fetch requests per shard, larger sizes prevent running out of memory because of too many concurrent fetch requests.

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Default:* 1048576

=== kafka_memory_share_for_fetch

The share of kafka subsystem memory that can be used for fetch read buffers, as a fraction of kafka subsystem memory amount

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* user

*Type:* number

*Default:* 0.5

=== kafka_mtls_principal_mapping_rules

Principal Mapping Rules for mTLS Authentication on the Kafka API

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* array

*Default:* None

=== kafka_nodelete_topics

Prevents the topics in the list from being deleted via the kafka api

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* array

*Default:* '{model::kafka_audit_logging_topic(), "__consumer_offsets", "_schemas"}'

=== kafka_noproduce_topics

Prevents the topics in the list from having message produced to them via the kafka api

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* array

*Default:* None

=== kafka_qdc_depth_alpha

Smoothing factor for kafka queue depth control depth tracking.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* number

*Default:* 0.8

=== kafka_qdc_depth_update_ms

Update frequency for kafka queue depth control.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '7s'

=== kafka_qdc_enable

Enable kafka queue depth control.

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* False

=== kafka_qdc_idle_depth

Queue depth when idleness is detected in kafka queue depth control.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 10

=== kafka_qdc_latency_alpha

Smoothing parameter for kafka queue depth control latency tracking.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* number

*Default:* 0.002

=== kafka_qdc_max_depth

Maximum queue depth used in kafka queue depth control.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 100

=== kafka_qdc_max_latency_ms

Max latency threshold for kafka queue depth control depth tracking.

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '80ms'

=== kafka_qdc_min_depth

Minimum queue depth used in kafka queue depth control.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 1

=== kafka_qdc_window_count

Number of windows used in kafka queue depth control latency tracking.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 12

=== kafka_qdc_window_size_ms

Window size for kafka queue depth control latency tracking.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '1500ms'

=== kafka_quota_balancer_min_shard_throughput_bps

The lowest value of the throughput quota a shard can get in the process of quota balancing, in bytes/s. 0 means there is no minimum.

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [-9223372036854775808, 9223372036854775807]

*Default:* 256

=== kafka_quota_balancer_min_shard_throughput_ratio

The lowest value of the throughput quota a shard can get in the process of quota balancing, expressed as a ratio of default shard quota. 0 means there is no minimum, 1 means no quota can be taken away by the balancer.

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* number

*Default:* 0.01

=== kafka_quota_balancer_node_period

Intra-node throughput quota balancer invocation period, in milliseconds. Value of 0 disables the balancer and makes all the throughput quotas immutable.

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '750ms'

=== kafka_quota_balancer_window

Time window used to average current throughput measurement for quota balancer, in milliseconds

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '5000ms'

=== kafka_request_max_bytes

Maximum size of a single request processed via Kafka API

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 4294967295]

*Default:* 104857600

=== kafka_rpc_server_stream_recv_buf

Userspace receive buffer max size in bytes

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Default:* None

=== kafka_rpc_server_tcp_recv_buf

Kafka server TCP receive buffer size in bytes.

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* None

*Type:* integer

*Accepted values:* [-2147483648, 2147483647]

*Default:* None

=== kafka_rpc_server_tcp_send_buf

Kafka server TCP transmit buffer size in bytes.

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* None

*Type:* integer

*Accepted values:* [-2147483648, 2147483647]

*Default:* None

=== kafka_sasl_max_reauth_ms

The maximum time between Kafka client reauthentications. If a client has not reauthenticated a connection within this time frame, that connection is torn down. Without this, a connection could live long after the client's credentials are expired or revoked. Session expiry is disabled if the value is null.

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* None

=== kafka_schema_id_validation_cache_capacity

Per-shard capacity of the cache for validating schema IDs.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 128

=== kafka_tcp_keepalive_idle_timeout_seconds

TCP keepalive idle timeout in seconds for kafka connections. This describes the timeout between tcp keepalive probes that the remote sitesuccessfully acknowledged. Refers to the TCP_KEEPIDLE socket option. When changed applies to new connections only.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17179869184, 17179869183]

*Default:* '120s'

=== kafka_tcp_keepalive_probe_interval_seconds

TCP keepalive probe interval in seconds for kafka connections. This describes the timeout between unacknowledged tcp keepalives. Refers to the TCP_KEEPINTVL socket option. When changed applies to new connections only.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17179869184, 17179869183]

*Default:* '60s'

=== kafka_tcp_keepalive_probes

TCP keepalive unacknowledged probes until the connection is considered dead for kafka connections. Refers to the TCP_KEEPCNT socket option. When changed applies to new connections only.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 4294967295]

*Default:* 3

=== kafka_throughput_control

List of throughput control groups that define exclusions from node-wide throughput limits. Each group consists of: ("name" (optional) - any unique group name, "client_id" - regex to match client_id). A connection is assigned the first matching group, then the connection is excluded from throughput control.

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* array

*Default:* None

=== kafka_throughput_controlled_api_keys

List of Kafka API keys that are subject to cluster-wide and node-wide throughput limit control

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* array

*Default:* ['produce', 'fetch']

=== kafka_throughput_limit_node_in_bps

Node wide throughput ingress limit - maximum kafka traffic throughput allowed on the ingress side of each node, in bytes/s. Default is no limit.

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* integer

*Accepted values:* [-9223372036854775808, 9223372036854775807]

*Default:* None

=== kafka_throughput_limit_node_out_bps

Node wide throughput egress limit - maximum kafka traffic throughput allowed on the egress side of each node, in bytes/s. Default is no limit.

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* integer

*Accepted values:* [-9223372036854775808, 9223372036854775807]

*Default:* None

=== kafka_throughput_replenish_threshold

Threshold for refilling the token bucket. Will be clamped between 1 and kafka_throughput_limit_node_*_bps.

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-9223372036854775808, 9223372036854775807]

*Default:* None

=== kafka_throughput_throttling_v2

Use throughput throttling based on a shared token bucket instead of balancing quota between shards

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* True

=== kvstore_flush_interval

Key-value store flush interval (ms)

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* 'std::chrono::milliseconds(10)'

=== kvstore_max_segment_size

Key-value maximum segment size (bytes)

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 16777216

=== leader_balancer_idle_timeout

Leadership rebalancing idle timeout

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '2min'

=== leader_balancer_mode

Leader balancer mode

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* #/definitions/model::leader_balancer_mode

*Default:* 'model::leader_balancer_mode::random_hill_climbing'

=== leader_balancer_mute_timeout

Leadership rebalancing mute timeout

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '5min'

=== leader_balancer_node_mute_timeout

Leadership rebalancing node mute timeout

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '20s'

=== leader_balancer_transfer_limit_per_shard

Per shard limit for in progress leadership transfers

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 512

=== legacy_group_offset_retention_enabled

Group offset retention is enabled by default in versions of Redpanda >= 23.1. To enable offset retention after upgrading from an older version set this option to true.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* False

=== legacy_permit_unsafe_log_operation

Permits the use of strings that may induct log injection/modification

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* True

=== legacy_unsafe_log_warning_interval_sec

Interval, in seconds, of how often a message informing the operator that unsafe strings are permitted

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [-17179869184, 17179869183]

*Default:* '300s'

=== log_cleanup_policy

Default topic cleanup policy

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* #/definitions/model::cleanup_policy_bitflags

*Default:* 'model::cleanup_policy_bitflags::deletion'

=== log_compaction_interval_ms

How often do we trigger background compaction

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '10s'

=== log_compaction_use_sliding_window

Use sliding window compaction.

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* True

=== log_compression_type

Default topic compression type

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* #/definitions/model::compression

*Default:* 'model::compression::producer'

=== log_disable_housekeeping_for_tests

Disables the housekeeping loop for local storage. The property exists to simplify testing and shouldn't be set in production.

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* False

=== log_message_timestamp_alert_after_ms

Threshold in milliseconds for alerting on messages with a timestamp after the broker's time, meaning they are in the future relative to the broker's clock.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '2h'

=== log_message_timestamp_alert_before_ms

Threshold in milliseconds for alerting on messages with a timestamp before the broker's time, meaning they are in the past relative to the broker's clock. null to disable this check

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* None

=== log_message_timestamp_type

Default topic messages timestamp type

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* #/definitions/model::timestamp_type

*Default:* 'model::timestamp_type::create_time'

=== log_retention_ms

delete segments older than this - default 1 week

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* #/definitions/retention_duration_property

*Default:* '7 * 24h'

=== log_segment_ms

Default log segment lifetime in ms for topics which do not set segment.ms

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* 'std::chrono::weeks{2}'

=== log_segment_ms_max

Upper bound on topic segment.ms: higher values will be clamped to this value

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '24h * 365'

=== log_segment_ms_min

Lower bound on topic segment.ms: lower values will be clamped to this value

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '10min'

=== log_segment_size

Default log segment size in bytes for topics which do not set segment.bytes

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 18446744073709551615]

*Default:* 134217728

=== log_segment_size_jitter_percent

Random variation to the segment size limit used for each partition

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 65535]

*Default:* 5

=== log_segment_size_max

Upper bound on topic segment.bytes: higher values will be clamped to this limit

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 18446744073709551615]

*Default:* None

=== log_segment_size_min

Lower bound on topic segment.bytes: lower values will be clamped to this limit

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 18446744073709551615]

*Default:* 1048576

=== max_compacted_log_segment_size

Max compacted segment size after consolidation

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 5368709120

=== max_concurrent_producer_ids

Max number of the active sessions (producers). When the threshold is passed Redpanda terminates old sessions. When an idle producer corresponding to the terminated session wakes up and produces - it leads to its batches being rejected with out of order sequence error.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 18446744073709551615]

*Default:* 'std::numeric_limits<uint64_t>::max()'

=== max_in_flight_pandaproxy_requests_per_shard

Maximum number of in flight HTTP requests permitted in pandaproxy per shard.  Any additional requests above this limit will be rejected with a 429 error

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 500

=== max_in_flight_schema_registry_requests_per_shard

Maximum number of in flight HTTP requests permitted in schema registry per shard.  Any additional requests above this limit will be rejected with a 429 error

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 500

=== max_kafka_throttle_delay_ms

Fail-safe maximum throttle delay on kafka requests

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* "30'000ms"

=== max_transactions_per_coordinator

Max number of the active txn sessions (producers). When the threshold is passed Redpanda terminates old sessions. When an idle producer corresponding to the terminated session wakes up and produces - it leads to its batches being rejected with invalid producer epoch or invalid_producer_id_mapping (it depends on the txn execution phase).

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 18446744073709551615]

*Default:* 'std::numeric_limits<uint64_t>::max()'

=== members_backend_retry_ms

Time between members backend reconciliation loop retries 

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '5s'

=== memory_abort_on_alloc_failure

If true, the redpanda process will terminate immediately when an allocation cannot be satisfied due to memory exhaustion. If false, an exception is thrown instead.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* True

=== metadata_dissemination_interval_ms

Interval for metadata dissemination batching

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* "3'000ms"

=== metadata_dissemination_retries

Number of attempts of looking up a topic's meta data like shard before failing a request

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-32768, 32767]

*Default:* 30

=== metadata_dissemination_retry_delay_ms

Delay before retry a topic lookup in a shard or other meta tables

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* "0'500ms"

=== metadata_status_wait_timeout_ms

Maximum time to wait in metadata request for cluster health to be refreshed

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '2s'

=== metrics_reporter_report_interval

cluster metrics reporter report interval

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '24h'

=== metrics_reporter_tick_interval

Cluster metrics reporter tick interval

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '1min'

=== metrics_reporter_url

cluster metrics reporter url

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* string

*Default:* 'https://m.rp.vectorized.io/v2'

=== minimum_topic_replication

Minimum permitted value of replication factor for new topics

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [-32768, 32767]

*Default:* 1

=== node_isolation_heartbeat_timeout

How long after the last heartbeat request a node will wait before considering itself to be isolated

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-9223372036854775808, 9223372036854775807]

*Default:* 3000

=== node_management_operation_timeout_ms

Timeout for executing node management operations

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '5s'

=== node_status_interval

Time interval between two node status messages. Node status messages establish liveness status outside of the Raft protocol.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '100ms'

=== node_status_reconnect_max_backoff_ms

Maximum backoff (in ms) to reconnect to an unresponsive peer during node status liveness checks.

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '15s'

=== oidc_clock_skew_tolerance

The amount of seconds to allow for when validating the exp, nbf, and iat claims in the token.

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [-17179869184, 17179869183]

*Default:* 'std::chrono::seconds{} * 30'

=== oidc_discovery_url

The URL pointing to the well-known discovery endpoint for the OIDC provider.

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* string

*Default:* 'https://auth.prd.cloud.redpanda.com/.well-known/openid-configuration'

=== oidc_keys_refresh_interval

The frequency of refreshing the JSON Web Keys (JWKS) used to validate access tokens.

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [-17179869184, 17179869183]

*Default:* '1h'

=== oidc_principal_mapping

Rule for mapping JWT Payload claim to a Redpanda User Principal

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* string

*Default:* '$.sub'

=== oidc_token_audience

A string representing the intended recipient of the token.

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* string

*Default:* 'redpanda'

=== partition_autobalancing_concurrent_moves

Number of partitions that can be reassigned at once

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 50

=== partition_autobalancing_max_disk_usage_percent

Disk usage threshold that triggers moving partitions from the node

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [0, 4294967295]

*Default:* 80

=== partition_autobalancing_min_size_threshold

Minimum size of partition that is going to be prioritized when rebalancing cluster due to disk size threshold being breached. By default this value is calculated automaticaly

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Default:* None

=== partition_autobalancing_mode

Partition autobalancing mode

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* #/definitions/model::partition_autobalancing_mode

*Default:* 'model::partition_autobalancing_mode::node_add'

=== partition_autobalancing_node_availability_timeout_sec

Node unavailability timeout that triggers moving partitions from the node

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [-17179869184, 17179869183]

*Default:* '15min'

=== partition_autobalancing_tick_interval_ms

Partition autobalancer tick interval

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '30s'

=== partition_autobalancing_tick_moves_drop_threshold

If the number of scheduled tick moves drops by this ratio, a new tick is scheduled immediately. Valid values are (0, 1]. For example, with a value of 0.2 and 100 scheduled moves in a tick, a new tick is scheduled when the inprogress moves are < 80.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* number

*Default:* 0.2

=== partition_manager_shutdown_watchdog_timeout

A threshold value to detect partitions which shutdown might have been stuck. After this threshold a watchdog in partition manager will log information about partition shutdown not making progress

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '30s'

=== pp_sr_smp_max_non_local_requests

Maximum number of x-core requests pending in Panda Proxy and Schema Registry seastar::smp group.  (for more details look at `seastar::smp_service_group` documentation)

*Requires Restart:* Yes

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 4294967295]

*Default:* None

=== quota_manager_gc_sec

Quota manager GC frequency in milliseconds

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* 'std::chrono::milliseconds(30000)'

=== raft_election_timeout_ms

Election timeout expressed in milliseconds

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* "1'500ms"

=== raft_enable_lw_heartbeat

enables raft optimization of heartbeats

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* True

=== raft_flush_timer_interval_ms

Interval of checking partition against the `raft_replica_max_pending_flush_bytes`

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '100ms'

=== raft_heartbeat_disconnect_failures

After how many failed heartbeats to forcibly close an unresponsive TCP connection.  Set to 0 to disable force disconnection.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 3

=== raft_heartbeat_interval_ms

Milliseconds for raft leader heartbeats

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* 'std::chrono::milliseconds(150)'

=== raft_heartbeat_timeout_ms

raft heartbeat RPC timeout

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '3s'

=== raft_io_timeout_ms

Raft I/O timeout

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* "10'000ms"

=== raft_learner_recovery_rate

Raft learner recovery rate limit in bytes per sec

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 104857600

=== raft_max_concurrent_append_requests_per_follower

Maximum number of concurrent append entries requests sent by leader to one follower

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 4294967295]

*Default:* 16

=== raft_max_recovery_memory

Max memory that can be used for reads in raft recovery process by default 15% of total memory

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Default:* None

=== raft_recovery_concurrency_per_shard

How many partitions may simultaneously recover data to a particular shard. This is limited to avoid overwhelming nodes when they come back online after an outage.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 64

=== raft_recovery_default_read_size

default size of read issued during raft follower recovery

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 524288

=== raft_recovery_throttle_disable_dynamic_mode

Disables dynamic rate allocation in recovery throttle (advanced).

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* False

=== raft_replica_max_pending_flush_bytes

Max not flushed bytes per partition. If configured threshold is reached log will automatically be flushed even though it wasn't explicitly requested

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Default:* 262144

=== raft_replicate_batch_window_size

Max size of requests cached for replication

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 1048576

=== raft_smp_max_non_local_requests

Maximum number of x-core requests pending in Raft seastar::smp group. (for more details look at `seastar::smp_service_group` documentation)

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 4294967295]

*Default:* None

=== raft_timeout_now_timeout_ms

Timeout for a timeout now request

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '1s'

=== raft_transfer_leader_recovery_timeout_ms

Timeout waiting for follower recovery when transferring leadership

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '10s'

=== readers_cache_eviction_timeout_ms

Duration after which inactive readers will be evicted from cache

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '30s'

=== reclaim_batch_cache_min_free

Free memory limit that will be kept by batch cache background reclaimer

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 67108864

=== reclaim_growth_window

Length of time in which reclaim sizes grow

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* "3'000ms"

=== reclaim_max_size

Maximum batch cache reclaim size

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 4194304

=== reclaim_min_size

Minimum batch cache reclaim size

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 131072

=== reclaim_stable_window

Length of time above which growth is reset

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* "10'000ms"

=== recovery_append_timeout_ms

Timeout for append entries requests issued while updating stale follower

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '5s'

=== release_cache_on_segment_roll

Free cache when segments roll

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* False

=== replicate_append_timeout_ms

Timeout for append entries requests issued while replicating entries

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '3s'

=== retention_bytes

Default max bytes per partition on disk before triggering a compaction

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* integer

*Default:* None

=== retention_local_strict

Trim log data when a cloud topic reaches its local retention limit. When this option is disabled Redpanda will allow partitions to grow past the local retention limit, and will be trimmed automatically as storage reaches the configured target size.

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* False

=== retention_local_strict_override

Trim log data when a cloud topic reaches its local retention limit. When this option is disabled Redpanda will allow partitions to grow past the local retention limit, and will be trimmed automatically as storage reaches the configured target size.

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* True

=== retention_local_target_bytes_default

Local retention size target for partitions of topics with cloud storage write enabled

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* integer

*Default:* None

=== retention_local_target_capacity_bytes

The target capacity in bytes that log storage will try to use before additional retention rules will take over to trim data in order to meet the target. When no target is specified storage usage is unbounded.

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* integer

*Accepted values:* [0, 18446744073709551615]

*Default:* None

=== retention_local_target_capacity_percent

The target capacity in percent of unreserved space (see disk_reservation_percent) that log storage will try to use before additional retention rules will take over to trim data in order to meet the target. When no target is specified storage usage is unbounded.

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* number

*Default:* 80.0

=== retention_local_target_ms_default

Local retention time target for partitions of topics with cloud storage write enabled

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '24h'

=== retention_local_trim_interval

The maximum amount of time before log storage will examine usage to determine of the target capacity has been exceeded and additional data trimming is required.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '30s'

=== retention_local_trim_overage_coeff

The space management control loop will reclaim the overage multiplied by this this coefficient in order to compensate for data that is written during the idle period between control loop invocations.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* number

*Default:* 2.0

=== rm_sync_timeout_ms

Time to wait state catch up before rejecting a request

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '10s'

=== rpc_client_connections_per_peer

The maximum number of connections a broker will open to each of its peers

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* integer

*Accepted values:* [-2147483648, 2147483647]

*Default:* 32

=== rpc_server_compress_replies

Enable compression for internal rpc server replies

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* False

=== rpc_server_listen_backlog

TCP connection queue length for Kafka server and internal RPC server

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* integer

*Accepted values:* [-2147483648, 2147483647]

*Default:* None

=== rpc_server_tcp_recv_buf

Internal RPC TCP receive buffer size in bytes.

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* None

*Type:* integer

*Accepted values:* [-2147483648, 2147483647]

*Default:* None

=== rpc_server_tcp_send_buf

Internal RPC TCP transmit buffer size in bytes.

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* None

*Type:* integer

*Accepted values:* [-2147483648, 2147483647]

*Default:* None

=== rps_limit_acls_and_users_operations

Rate limit for controller acls and users operations

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 1000

=== rps_limit_configuration_operations

Rate limit for controller configuration operations

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 1000

=== rps_limit_move_operations

Rate limit for controller move operations

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 1000

=== rps_limit_node_management_operations

Rate limit for controller node management operations

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 1000

=== rps_limit_topic_operations

Rate limit for controller topic operations

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 1000

=== sampled_memory_profile

If true, memory allocations will be sampled and tracked. A sampled live set of allocations can then be retrieved from the Admin API. Additionally, we will periodically log the top-n allocation sites

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* True

=== sasl_kerberos_config

The location of the Kerberos krb5.conf file for Redpanda

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* string

*Default:* '/etc/krb5.conf'

=== sasl_kerberos_keytab

The location of the Kerberos keytab file for Redpanda

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* string

*Default:* '/var/lib/redpanda/redpanda.keytab'

=== sasl_kerberos_principal

The primary of the Kerberos Service Principal Name (SPN) for Redpanda

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* string

*Default:* 'redpanda'

=== sasl_kerberos_principal_mapping

Rules for mapping Kerberos Principal Names to Redpanda User Principals

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* array

*Default:* ['DEFAULT']

=== sasl_mechanisms

A list of supported SASL mechanisms. `SCRAM`, `GSSAPI`, and `OAUTHBEARER` are allowed.

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* array

*Default:* ['SCRAM']

=== segment_appender_flush_timeout_ms

Maximum delay until buffered data is written

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* 'std::chrono::milliseconds(1s)'

=== segment_fallocation_step

Size for segments fallocation

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 33554432

=== space_management_enable

Enable automatic space management.

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* True

=== space_management_enable_override

Enable automatic space management. This option is ignored and deprecated in versions >= v23.3.

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* False

=== space_management_max_log_concurrency

Maximum parallel logs inspected during space management process.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 65535]

*Default:* 20

=== space_management_max_segment_concurrency

Maximum parallel segments inspected during space management process.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 65535]

*Default:* 10

=== storage_compaction_index_memory

Maximum number of bytes that may be used on each shard by compactionindex writers

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 18446744073709551615]

*Default:* 134217728

=== storage_compaction_key_map_memory

Maximum number of bytes that may be used on each shard by compaction key-offset maps. Only respected when `log_compaction_use_sliding_window` is true.

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 18446744073709551615]

*Default:* 134217728

=== storage_compaction_key_map_memory_limit_percent

Limit on `storage_compaction_key_map_memory`, expressed as a percentage of memory per shard, that bounds the amount of memory used by compaction key-offset maps. NOTE: Memory per shard is computed after `wasm_per_core_memory_reservation`. Only respected when `log_compaction_use_sliding_window` is true.

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* tunable

*Type:* number

*Default:* 12.0

=== storage_ignore_cstore_hints

if set, cstore hints will be ignored and will not be used for data access (but will otherwise be generated)

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* False

=== storage_ignore_timestamps_in_future_sec

If set, timestamps more than this many seconds in the future relative tothe server's clock will be ignored for data retention purposes, and retention will act based on another timestamp in the same segment, or the mtime of the segment file if no valid timestamp is available

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17179869184, 17179869183]

*Default:* None

=== storage_max_concurrent_replay

Maximum number of partitions' logs that will be replayed concurrently at startup, or flushed concurrently on shutdown.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 18446744073709551615]

*Default:* 1024

=== storage_min_free_bytes

Threshold of minimum bytes free space before rejecting producers.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 5368709120

=== storage_read_buffer_size

Size of each read buffer (one per in-flight read, per log segment)

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 131072

=== storage_read_readahead_count

How many additional reads to issue ahead of current read location

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-32768, 32767]

*Default:* 10

=== storage_reserve_min_segments

The number of segments per partition that the system will attempt to reserve disk capacity for. For example, if the maximum segment size is configured to be 100 MB, and the value of this option is 2, then in a system with 10 partitions Redpanda will attempt to reserve at least 2 GB of disk space.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-32768, 32767]

*Default:* 2

=== storage_space_alert_free_threshold_bytes

Threshold of minimum bytes free space before setting storage space alert

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 0

=== storage_space_alert_free_threshold_percent

Threshold of minimum percent free space before setting storage space alert

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 4294967295]

*Default:* 5

=== storage_strict_data_init

Requires that an empty file named `.redpanda_data_dir` be present in the data directory. Redpanda will refuse to start if it is not found.

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* boolean

*Default:* False

=== storage_target_replay_bytes

Target bytes to replay from disk on startup after clean shutdown: controls frequency of snapshots and checkpoints

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 18446744073709551615]

*Default:* 10737418240

=== superusers

List of superuser usernames

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* string

*Default:* None

=== target_fetch_quota_byte_rate

Target fetch size quota byte rate (bytes per second) - disabled default

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* user

*Type:* integer

*Accepted values:* [0, 4294967295]

*Default:* None

=== target_quota_byte_rate

Target request size quota byte rate (bytes per second) - 2GB default

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [0, 4294967295]

*Default:* 2147483648

=== tm_sync_timeout_ms

Time to wait state catch up before rejecting a request

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '10s'

=== topic_fds_per_partition

Required file handles per partition when creating topics

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-2147483648, 2147483647]

*Default:* 5

=== topic_memory_per_partition

Required memory per partition when creating topics

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* tunable

*Type:* integer

*Default:* 4194304

=== topic_partitions_per_shard

Maximum number of partitions which may be allocated to one shard (CPU core)

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 4294967295]

*Default:* 1000

=== topic_partitions_reserve_shard0

Reserved partition slots on shard (CPU core) 0 on each node.  If this is >= topic_partitions_per_core, no data partitions will be scheduled on shard 0

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 4294967295]

*Default:* 2

=== transaction_coordinator_cleanup_policy

Cleanup policy for a transaction coordinator topic

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* #/definitions/model::cleanup_policy_bitflags

*Default:* 'model::cleanup_policy_bitflags::deletion'

=== transaction_coordinator_delete_retention_ms

delete segments older than this - default 1 week

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '10080min'

=== transaction_coordinator_log_segment_size

How large in bytes should each log segment be (default 1G)

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [0, 18446744073709551615]

*Default:* 1073741824

=== transaction_coordinator_partitions

Amount of partitions for transactions coordinator

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-2147483648, 2147483647]

*Default:* 50

=== transactional_id_expiration_ms

Producer ids are expired once this time has elapsed after the last write with the given producer id.

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '10080min'

=== tx_log_stats_interval_s

How often to log per partition tx stats, works only with debug logging enabled.

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17179869184, 17179869183]

*Default:* '10s'

=== tx_timeout_delay_ms

Delay before scheduling next check for timed out transactions

*Requires Restart:* No

*Nullable:* No

*Visibility:* user

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '1000ms'

=== usage_disk_persistance_interval_sec

The interval in which all usage stats are written to disk

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17179869184, 17179869183]

*Default:* 'std::chrono::seconds(60 * 5)'

=== usage_num_windows

The number of windows to persist in memory and disk

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 24

=== usage_window_width_interval_sec

The width of a usage window, tracking cloud and kafka ingress/egress traffic each interval

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17179869184, 17179869183]

*Default:* 'std::chrono::seconds(3600)'

=== use_fetch_scheduler_group

Use a separate scheduler group for fetch processing

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* boolean

*Default:* True

=== wait_for_leader_timeout_ms

Timeout (ms) to wait for leadership in metadata cache

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* "5'000ms"

=== zstd_decompress_workspace_bytes

Size of the zstd decompression workspace

*Requires Restart:* No

*Nullable:* No

*Visibility:* tunable

*Type:* integer

*Default:* 8388608



== Kafka Client

Kafka Client intro

=== broker_tls

TLS configuration for the brokers

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* #/definitions/config::tls_config

*Default:* 'config::tls_config()'

=== brokers

List of address and port of the brokers

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* array

*Default:* 'std::vector<net::unresolved_address>({{"127.0.0.1", 9092}})'

=== client_identifier

Identifier to use within the kafka request header

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* None

*Type:* string

*Default:* 'test_client'

=== consumer_heartbeat_interval

Interval (in milliseconds) for consumer heartbeats

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '500ms'

=== consumer_rebalance_timeout

Timeout (in milliseconds) for consumer rebalance

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '2s'

=== consumer_request_max_bytes

Max bytes to fetch per request

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* integer

*Accepted values:* [-2147483648, 2147483647]

*Default:* 1048576

=== consumer_request_min_bytes

Min bytes to fetch per request

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* integer

*Accepted values:* [-2147483648, 2147483647]

*Default:* 1

=== consumer_request_timeout

Interval (in milliseconds) for consumer request timeout

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '100ms'

=== consumer_session_timeout

Timeout (in milliseconds) for consumer session

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '10s'

=== produce_ack_level

Number of acknowledgments the producer requires the leader to have received before considering a request complete, choices are 0, 1 and -1

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* integer

*Accepted values:* [-32768, 32767]

*Default:* -1

=== produce_batch_delay

Delay (in milliseconds) to wait before sending batch

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '100ms'

=== produce_batch_record_count

Number of records to batch before sending to broker

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* integer

*Accepted values:* [-2147483648, 2147483647]

*Default:* 1000

=== produce_batch_size_bytes

Number of bytes to batch before sending to broker

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* integer

*Accepted values:* [-2147483648, 2147483647]

*Default:* 1048576

=== produce_compression_type

Enable or disable compression by the kafka client. Specify 'none' to disable compression or one of the supported types [gzip, snappy, lz4, zstd]

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* string

*Default:* 'none'

=== produce_shutdown_delay

Delay (in milliseconds) to allow for final flush of buffers before shutting down

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '0ms'

=== retries

Number of times to retry a request to a broker

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* integer

*Default:* 5

=== retry_base_backoff

Delay (in milliseconds) for initial retry backoff

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '100ms'

=== sasl_mechanism

The SASL mechanism to use when connecting

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* string

*Default:* ''

=== scram_password

Password to use for SCRAM authentication mechanisms

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* string

*Default:* ''

=== scram_username

Username to use for SCRAM authentication mechanisms

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* string

*Default:* ''



== HTTP Proxy

HTTP Proxy intro

=== advertised_pandaproxy_api

Rest API address and port to publish to client

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* #/definitions/model::broker_endpoint

*Default:* None

=== api_doc_dir

API doc directory

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* string

*Default:* '/usr/share/redpanda/proxy-api-doc'

=== client_cache_max_size

The maximum number of kafka clients in the LRU cache

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* None

*Type:* integer

*Default:* 10

=== client_keep_alive

Time in milliseconds that an idle connection may remain open

*Requires Restart:* Yes

*Nullable:* No

*Visibility:* None

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* '5min'

=== consumer_instance_timeout

How long to wait for an idle consumer before removing it

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* integer

*Accepted values:* [-17592186044416, 17592186044415]

*Default:* 'std::chrono::minutes{5}'

=== pandaproxy_api

Rest API listen address and port

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* #/definitions/config::rest_authn_endpoint

*Default:* {'address': 'net::unresolved_address("0.0.0.0", 8082)', 'authn_method': 'std::nullopt'}

=== pandaproxy_api_tls

TLS configuration for Pandaproxy api

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* #/definitions/config::endpoint_tls_config

*Default:* None



== Schema Registry

Schema Registry intro

=== schema_registry_api

Schema Registry API listen address and port

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* #/definitions/config::rest_authn_endpoint

*Default:* {'address': 'net::unresolved_address("0.0.0.0", 8081)', 'authn_method': 'std::nullopt'}

=== schema_registry_api_tls

TLS configuration for Schema Registry API

*Requires Restart:* No

*Nullable:* No

*Visibility:* None

*Type:* #/definitions/config::endpoint_tls_config

*Default:* None

=== schema_registry_replication_factor

Replication factor for internal _schemas topic.  If unset, defaults to `default_topic_replication`

*Requires Restart:* No

*Nullable:* Yes

*Visibility:* None

*Type:* integer

*Accepted values:* [-32768, 32767]

*Default:* None

